{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorin\\.conda\\envs\\AGI\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "model_name = \"jeggers/OpenELM-270M-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add hidden_size attribute to the model config\n",
    "# take value from model_dim attribute in config\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.09G/1.09G [12:13<00:00, 1.48MB/s]   \n",
      "c:\\Users\\jorin\\.conda\\envs\\AGI\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jorin\\.cache\\huggingface\\hub\\models--jeggers--OpenELM-270M-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.19MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jeggers/OpenELM-270M-Instruct/commit/1d0c39b78a165d32275707622fcd2fef721ce8e8', commit_message='Upload tokenizer', commit_description='', oid='1d0c39b78a165d32275707622fcd2fef721ce8e8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push to hub\n",
    "new_name = \"jeggers/OpenELM-270M-Instruct\"\n",
    "model.push_to_hub(new_name)\n",
    "tokenizer.push_to_hub(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorin\\.conda\\envs\\AGI\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "WARNING:root:A <class 'transformers_modules.apple.OpenELM-270M-Instruct.eb111ff2e6724348e5b905984063d4064d4bc579.modeling_openelm.OpenELMForCausalLM'> model is loaded from 'jeggers/OpenELM-270M-Instruct', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "# try to load new model with trl\n",
    "\n",
    "trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(new_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,     0,     0,     0,     1,   697,  1023,  2211,  3023],\n",
      "        [    0,     0,     0,     0,     0,     0,     1,   319,  4996],\n",
      "        [    1,   450,  7339, 29888, 19467,   310,   319, 29902,   526]]), 'attention_mask': tensor([[0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# generate some text\n",
    "input_texts = [\"one two three four\", \"A quick\", \"The godfathers of AI are\"]\n",
    "batch = tokenizer(input_texts, padding=\"longest\", return_tensors=\"pt\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one two three four five six seven eight ninety o thirties hundred sixties eighties seventies\\n-', 'A quick reference guide to the 20 most common types of adware applications, categorized by name or', \"The godfathers of AI are meeting in Israel on Saturday, August 25 for the sixth meeting of the world's most\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out = trl_model.generate(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], max_new_tokens=20, do_sample=True)\n",
    "out_texts = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "print(out_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorin\\.conda\\envs\\AGI\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token, tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.clean_up_tokenization_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "1. Let S < a. B ≥ T.\n",
    "2. By P(T)≥a + P(S). B ≥ P(B) → By⊆P(B) + P(S)→by⊆−P(S) → Step-by-step: a+B≥ a ≤ a. Step-by-step: a∈S≥0.Proof of Theorem1.Proof of Theorem1.Proof of Theorem1.proof of Theorem1.\n",
    "[37]: a. 3(b. a.)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "\n",
      "1. Let S < a. B ≥ T.\n",
      "2. By P(T)≥a + P(S). B ≥ P(B) → By⊆P(B) + P(S)→by⊆−P(S) → Step-by-step: a+B≥ a ≤ a. Step-by-step: a∈S≥0.Proof of Theorem1.Proof of Theorem1.Proof of Theorem1.proof of Theorem1.\n",
      "[37]: a. 3(b. a.)\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "toks = tokenizer(text, return_tensors=\"pt\")\n",
    "print(len(toks[\"input_ids\"][0]))\n",
    "# decode and encode again\n",
    "text = tokenizer.decode(toks[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(text)\n",
    "toks = tokenizer(text, return_tensors=\"pt\")\n",
    "print(len(toks[\"input_ids\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([396])\n",
      "tensor(200)\n",
      "<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Step', '-', 'by', '-', 'Step', ':', ' Step', '-', 'by', 'Step', '\\n', 'Str', 'ategy', ':', '\\n', 'Answer', ':', ' Step', '-', 'by', '-', 'Step', '-', 'Step', ' Step', '-', 'by', 'Step', '-', 'Str', 'ategy', '=', 'Step', '-', 'by', '-', 'Step', ' Step', '-', 'by', 'step', '-', 'Step', '-', 'Str', 'ategy', '=', 'Step', '-', 'by', '-', 'Step', ' Step', '-', 'by', '-', 'Step', '-', 'Str', 'ategy', 'Step', ' ', '=', ' Step', '-', 'Str', 'ategy', 'Step', '-', 'Str', 'ategy', 'Step', '-', 'Str', 'ategy', 'Str', 'ategy', 'Str', 'ategy', 'Str', 'ategy', 'Str', 'ategy', 'Solution', 'Step', '-', 'Str', 'ategy', 'Solution', 'Str', 'ategy', 'Solution', 'Solution', ' Solution', 'By', 'Sub', 'stitution', 'Str', 'ategy', 'Str', 'ategy', 'Str', 'ategy', 'Str', 'ategy', 'Str', 'ategy', 'Str', 'ategy', 'Str', 'ategy', 'Str', 'ategy', 'Str', 'ategy', 'Str', 'ategy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', 'onomy', 'Str', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "<class 'list'>\n",
      "0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "        10728,    35,  2182,    35, 10728,    48, 10520,    35,  2182, 10728,\n",
    "          221,  6153, 28508,    48,   221,  6402,    48, 10520,    35,  2182,\n",
    "           35, 10728,    35, 10728, 10520,    35,  2182, 10728,    35,  6153,\n",
    "        28508,    51, 10728,    35,  2182,    35, 10728, 10520,    35,  2182,\n",
    "         6324,    35, 10728,    35,  6153, 28508,    51, 10728,    35,  2182,\n",
    "           35, 10728, 10520,    35,  2182,    35, 10728,    35,  6153, 28508,\n",
    "        10728,   243,    51, 10520,    35,  6153, 28508, 10728,    35,  6153,\n",
    "        28508, 10728,    35,  6153, 28508,  6153, 28508,  6153, 28508,  6153,\n",
    "        28508,  6153, 28508, 35979, 10728,    35,  6153, 28508, 35979,  6153,\n",
    "        28508, 35979, 35979, 13901,  4533,  9095, 26506,  6153, 28508,  6153,\n",
    "        28508,  6153, 28508,  6153, 28508,  6153, 28508,  6153, 28508,  6153,\n",
    "        28508,  6153, 28508,  6153, 28508,  6153, 28508,  6153, 12131,  6153,\n",
    "        12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153,\n",
    "        12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153,\n",
    "        12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153,\n",
    "        12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153,\n",
    "        12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153,\n",
    "        12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153,\n",
    "        12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153,\n",
    "        12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153, 12131,  6153,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "           -1,    -1,    -1,    -1,    -1,    -1])\n",
    "print(x.size())\n",
    "# count number of greater than 0\n",
    "print((x > 0).sum())\n",
    "\n",
    "# replace -1 with 1\n",
    "x = x.masked_fill(x < 0, 1)\n",
    "# test tokenizer\n",
    "text  = tokenizer.batch_decode(x, skip_special_tokens=True)\n",
    "print(type(tokenizer))\n",
    "print(text)\n",
    "print(type(text))\n",
    "toks = tokenizer(text, return_tensors=\"pt\")\n",
    "print(len(toks[\"input_ids\"][0]))\n",
    "text2 = tokenizer.decode(toks[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(text2==text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AGI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
