{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# fine tune the model on the CoT dataset\n",
    "# train only for output tokens in the dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import wandb\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "model_name = \"facebook/galactica-125m\"\n",
    "tokenizer_name = model_name\n",
    "\n",
    "dataset_name = \"jeggers/CoT-Collection-finetuning\"\n",
    "input_column = \"final_input\"\n",
    "output_column = \"final_target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a26d2db2954601a998536202b987ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/649 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2fdbb4e8ea4b3e8a1b39692466822e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/73 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 125,325,312 || trainable%: 0.2353\n"
     ]
    }
   ],
   "source": [
    "# load model, tokenizer and dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# filter dataset to only contain samples that do not stat with 'ANSWER:'\n",
    "dataset = dataset.filter(lambda x: not x[output_column].startswith(\"ANSWER:\"))\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# create new 'all' column that contains both input and output tokens\n",
    "def combine_input_output(batch):\n",
    "    return {\"all\": [x + \" \" + y + \"</s>\" for x, y in zip(batch[input_column], batch[output_column])]}\n",
    "dataset = dataset.map(combine_input_output, batched=True, batch_size=-1)\n",
    "\n",
    "# convert to peft model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Challenge yourself with this crossword clue:\\n'\n",
      " 'Clue given: Relative by marriage\\n'\n",
      " 'Year published: 2003\\n'\n",
      " 'Length of answer: 5\\n'\n",
      " 'Primary hint: The 3rd letter is L\\n'\n",
      " 'Secondary hint: The 1st letter is I\\n'\n",
      " 'Provide the answer in all caps.\\n'\n",
      " 'BOT:  1. The answer is a 5-letter word for a relative by marriage.\\n'\n",
      " '2. The 3rd letter is L and the 1st letter is I.\\n'\n",
      " '3. Common in-law relatives: mother, father, sister, brother.\\n'\n",
      " '4. \"In-law\" is often shortened to just \"-law\" in crosswords.\\n'\n",
      " '5. The only 5-letter in-law that fits is \"inlaw\" (informal spelling).\\n'\n",
      " '\\n'\n",
      " 'ANSWER: INLAW</s>',\n",
      " 'When you wrongly multiplied a number that should have been divided by 2.4, '\n",
      " 'you got 288. What is the correctly calculated value divided by 5?\\n'\n",
      " 'BOT:  1. Identify the incorrect calculation: number * 2.4 = 288\\n'\n",
      " '2. Find the correct number: 288 / 2.4 = 120\\n'\n",
      " '3. Perform the correct calculation: 120 / 2.4 = 50\\n'\n",
      " '4. Divide the result by 5: 50 / 5 = 10\\n'\n",
      " '\\n'\n",
      " 'ANSWER: 10</s>',\n",
      " 'Look at the two sentences provided below. Identify the common word between '\n",
      " 'the two sentences and determine whether it has the same meaning in both '\n",
      " \"contexts. Respond with 'True' or 'False'.\\n\"\n",
      " '\\n'\n",
      " 'Sentence 1: The sequence of names was alphabetical.\\n'\n",
      " '\\n'\n",
      " 'Sentence 2: He invented a technique to determine the sequence of base pairs '\n",
      " 'in DNA.\\n'\n",
      " 'BOT:  The common word in both sentences is \"sequence.\"\\n'\n",
      " '\\n'\n",
      " 'In the first sentence, \"sequence\" refers to an ordered arrangement of names '\n",
      " 'based on the alphabet.\\n'\n",
      " '\\n'\n",
      " 'In the second sentence, \"sequence\" refers to the specific order of base '\n",
      " 'pairs in a DNA molecule.\\n'\n",
      " '\\n'\n",
      " 'Both uses of \"sequence\" convey the idea of a particular order or arrangement '\n",
      " 'of elements.\\n'\n",
      " '\\n'\n",
      " 'ANSWER: True</s>',\n",
      " 'What are the main metabolites of alcohol metabolism?\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'A: Glucose, fructose,\\n'\n",
      " 'B: Glycerol, fatty acids,\\n'\n",
      " 'C: Acetone, lactate\\n'\n",
      " 'D: Aspartate, glutamate.\\n'\n",
      " 'E: Pyruvate, beta-hydroxybutyrate,\\n'\n",
      " 'F: Glyceraldehyde-3-phosphate, 1,3-bisphosphoglycerate,\\n'\n",
      " 'G: Acetyl-CoA, lipids,\\n'\n",
      " 'H: Acetaldehyde, acetate,\\n'\n",
      " 'I: Succinate, fumarate,\\n'\n",
      " 'J: Citrate, oxaloacetate,\\n'\n",
      " '\\n'\n",
      " 'Indicate your choice by typing the letter only.\\n'\n",
      " 'BOT:  Alcohol metabolism primarily occurs in the liver through a two-step '\n",
      " 'process:\\n'\n",
      " '\\n'\n",
      " '1. Alcohol is first oxidized to acetaldehyde by the enzyme alcohol '\n",
      " 'dehydrogenase.\\n'\n",
      " '2. Acetaldehyde is then further oxidized to acetate by aldehyde '\n",
      " 'dehydrogenase.\\n'\n",
      " '\\n'\n",
      " 'The main metabolites in this process are:\\n'\n",
      " '- Acetaldehyde: the intermediate product\\n'\n",
      " '- Acetate: the final product\\n'\n",
      " '\\n'\n",
      " 'These metabolites are listed in option H.\\n'\n",
      " '\\n'\n",
      " 'ANSWER: H</s>',\n",
      " 'Work out the math problem and present only the final result. Problem: While '\n",
      " 'playing a trivia game, Team A scored 2 points, Team B scored 9 points and '\n",
      " 'Team C scored 4 points. How many points were scored total?\\n'\n",
      " 'BOT:  2 points (Team A) + 9 points (Team B) + 4 points (Team C) = 15 points '\n",
      " 'total\\n'\n",
      " '\\n'\n",
      " 'ANSWER: 15</s>']\n"
     ]
    }
   ],
   "source": [
    "# print first 5 examples\n",
    "from pprint import pprint\n",
    "pprint(dataset[\"train\"][\"all\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['final_input', 'final_target', 'all'],\n",
      "        num_rows: 649\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['final_input', 'final_target', 'all'],\n",
      "        num_rows: 73\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = 1\n",
    "if tokenizer.eos_token_id is None:\n",
    "    tokenizer.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077a65c6f3504ad8b91f45fe64b1b264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8e6983361643c38b23d94e84c69d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['final_input', 'final_target', 'all', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 648\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['final_input', 'final_target', 'all', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 72\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    examples = tokenizer(examples['all'], padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].clone()\n",
    "    return examples\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, batch_size=batch_size, drop_last_batch=True)\n",
    "\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8996, 417, 286, 1231, 8650, 299, 6445, 4817, 53, 221, 221, 221, 55, 48, 22254, 34, 25703, 34, 221, 56, 48, 11041, 11658, 34, 6577, 4698, 34, 221, 57, 48, 21122, 739, 34, 17243, 221, 58, 48, 995, 20754, 34, 13838, 36, 221, 59, 48, 6486, 17582, 34, 5110, 35, 6981, 43101, 34, 221, 60, 48, 11041, 1102, 15875, 35, 41, 35, 9273, 34, 243, 39, 34, 41, 35, 14028, 5419, 47655, 405, 34, 221, 61, 48, 40801, 35, 17535, 34, 11512, 34, 221, 62, 48, 1587, 5614, 8208, 34, 10422, 34, 221, 63, 48, 348, 29813, 405, 34, 44312, 405, 34, 221, 64, 48, 26246, 3156, 34, 1540, 17173, 18572, 34, 221, 221, 5164, 7496, 5005, 3715, 404, 19902, 286, 11905, 869, 36, 221, 56, 4675, 48, 243, 24647, 4817, 6853, 4044, 301, 286, 3639, 1100, 281, 682, 35, 6324, 922, 48, 221, 221, 39, 36, 24647, 343, 935, 15833, 321, 4708, 15875, 404, 286, 4674, 6445, 12973, 36, 221, 40, 36, 1587, 5614, 8208, 343, 967, 1653, 15833, 321, 10422, 404, 29483, 12973, 36, 221, 221, 592, 1231, 8650, 301, 495, 922, 417, 48, 221, 35, 1587, 5614, 8208, 48, 286, 5275, 2792, 221, 35, 21122, 405, 48, 286, 2577, 2792, 221, 221, 7384, 8650, 417, 6721, 301, 7872, 414, 36, 221, 221, 2034, 11677, 1409, 48, 414, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# print tensor of first sample\n",
    "print(tokenized_datasets[\"train\"][\"input_ids\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(\"cuda\")\n",
    "labels = inputs[\"input_ids\"].clone()\n",
    "del inputs[\"token_type_ids\"]\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "print(outputs.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if \"loss\" in logs:\n",
    "            wandb.log({\"training_loss\": logs[\"loss\"]})\n",
    "        if \"eval_loss\" in logs:\n",
    "            wandb.log({\"eval_loss\": logs[\"eval_loss\"]})\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"your-name/bigscience/mt0-large-lora\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    # report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    # callbacks=[LoggingCallback()],\n",
    "    # data_collator=collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='162' max='162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [162/162 00:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.033500</td>\n",
       "      <td>3.040254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=162, training_loss=3.317157863098898, metrics={'train_runtime': 15.1744, 'train_samples_per_second': 85.407, 'train_steps_per_second': 10.676, 'total_flos': 339808610156544.0, 'train_loss': 3.317157863098898, 'epoch': 2.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# merge adapter weights into model\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441364ee5a034c63898ec0a8c0f8b0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/500M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jeggers/galactica-125m-cot-only/commit/84197be6cf1a4859eae3120245a8640f4d47d771', commit_message='Upload OPTForCausalLM', commit_description='', oid='84197be6cf1a4859eae3120245a8640f4d47d771', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"jeggers/galactica-125m-cot-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12199e94daa64f789ab86fc6fde45e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jeggers/galactica-125m-cot-only/commit/52cf05c53d8a149ab8a19c91c8a517d9918f198b', commit_message='Upload tokenizer', commit_description='', oid='52cf05c53d8a149ab8a19c91c8a517d9918f198b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"jeggers/galactica-125m-cot-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x000001FFFBA033D0>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 20238e9c250, raw_cell=\"question = \"Pick the optimal word to complete the ..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/d%3A/Google%20Drive/Other%20computers/My%20laptop/Daten/Hobbys/Informatik/Mashine%20leaning/ThinkingLLMs/finetune_model.ipynb#X16sZmlsZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pick the optimal word to complete the phrase: Natalie was trying to fit a prom dress over their client\\'s bust, but struggled to do so because the _ was too big. A: dress B: bust Provide your answer using the letter only. BOT: 1. Attend to the phrase: Natalie was trying to fit a prom dress over her client\\'s bust.\\n2. As the question shows, the word and sentence are separated, the first sentence is a word of length 2.\\n3. The sentence is meant to complete the phrase: \"Try to work out a prom dress on the client\\'s bust.\"\\n4. The word \"B\" must refer to the large bust.\\n5. The sentence points to a large bust.\\n\\nANSWER: B� expendit']\n",
      "tensor([[32893,   286,  2936,  5104,   321,  2856,   286, 20459,    48, 38703,\n",
      "          1069,   435, 13571,   321,  2422,   281,  1928, 35998,   797,   817,\n",
      "         10753,    29,   105,   311,   690,    34,   835, 20633,  1663,   321,\n",
      "           917,   891,  1543,   286,   243,    85,   435,  4606,  6499,    36,\n",
      "           351,    48, 35998,   410,    48,   311,   690, 10564,   573,  5005,\n",
      "          6692,   672,   286, 11905,   869,    36,   410,  4675,    48,   243,\n",
      "            39,    36,  9162,   493,   321,   286, 20459,    48, 38703,  1069,\n",
      "           435, 13571,   321,  2422,   281,  1928, 35998,   797,  2486, 10753,\n",
      "            29,   105,   311,   690,    36,   221,    40,    36,   995,   286,\n",
      "          2882,  1586,    34,   286,  5104,   312,  9553,   417,  6076,    34,\n",
      "           286,   935,  9553,   343,   281,  5104,   299,  1829,   243,    40,\n",
      "            36,   221,    41,    36,   381,  9553,   343, 17107,   321,  2856,\n",
      "           286, 20459,    48,   243,    24,    74,   701,   321,  1032,   758,\n",
      "           281,  1928, 35998,   377,   286, 10753,    29,   105,   311,   690,\n",
      "            36,    24,   221,    42,    36,   381,  5104,   243,    24,    56,\n",
      "            24,  2216,  3553,   321,   286,  1263,   311,   690,    36,   221,\n",
      "            43,    36,   381,  9553,  1939,   321,   281,  1263,   311,   690,\n",
      "            36,   221,   221,  2034, 11677,  1409,    48,   410,   200, 18218]],\n",
      "       device='cuda:0')\n",
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x000001FFFBA033D0>> (for post_run_cell), with arguments args (<ExecutionResult object at 20238e9cb90, execution_count=28 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 20238e9c250, raw_cell=\"question = \"Pick the optimal word to complete the ..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/d%3A/Google%20Drive/Other%20computers/My%20laptop/Daten/Hobbys/Informatik/Mashine%20leaning/ThinkingLLMs/finetune_model.ipynb#X16sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "question = \"Pick the optimal word to complete the phrase: Natalie was trying to fit a prom dress over their client's bust, but struggled to do so because the _ was too big. A: dress B: bust Provide your answer using the letter only.\"\n",
    "inputs = tokenizer(question + \" BOT: \", return_tensors=\"pt\").to(\"cuda\")\n",
    "del inputs[\"token_type_ids\"]\n",
    "out = model.generate(**inputs, max_new_tokens=120, num_return_sequences=1, do_sample=True, temperature=0.8)\n",
    "text = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "print(text)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
